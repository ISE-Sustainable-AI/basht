# Source: ray/templates/raycluster.yaml
apiVersion: cluster.ray.io/v1
kind: RayCluster
metadata:
  name: ray-cluster
spec:
  # The maximum number of workers nodes to launch in addition to the head node.
  maxWorkers: 3
  # The autoscaler will scale up the cluster faster with higher upscaling speed.
  # E.g., if the task requires adding more nodes then autoscaler will gradually
  # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
  # This number should be > 0.
  upscalingSpeed: 1.0
  # If a node is idle for this many minutes, it will be removed.
  idleTimeoutMinutes: 5
  # Specify the pod type for the ray head node (as configured below).
  headPodType: rayHeadType
  # Specify the allowed pod types for this ray cluster and the resources they provide.
  podTypes:
    - name: rayHeadType
      minWorkers: 0
      maxWorkers: 0
      podConfig:
        apiVersion: v1
        kind: Pod
        metadata:
          generateName: ray-head-type-
        spec:
          restartPolicy: Never
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumes:
            - name: dshm
              #nfs:
              #  server: 10.98.230.88
              #  path: /ray-dshm
              emptyDir:
                medium: Memory
            - name: ray-results
              nfs:
                server: $nfs_server
                path: $nfs_path
          containers:
            - name: ray-node
              #imagePullPolicy: Always
              image: vdocker2603/ml-benchmark-raytune-k8s
              # Do not change this command - it keeps the pod alive until it is
              # explicitly killed.
              command: ["/bin/bash", "-c", "--"]
              args: ["trap : TERM INT; sleep infinity & wait;"]
              env:
                - name: RAY_gcs_server_rpc_server_thread_num
                  value: "1"
                - name: "METRICS_STORAGE_HOST"
                  value: "$metrics_ip"
              ports:
                - containerPort: 6379 # Redis port for Ray <= 1.10.0. GCS server port for Ray >= 1.11.0.
                - containerPort: 10001 # Used by Ray Client
                - containerPort: 8265 # Used by Ray Dashboard
                - containerPort: 8000 # Used by Ray Serve

              # This volume allocates shared memory for Ray to use for its plasma
              # object store. If you do not provide this, Ray will fall back to
              # /tmp which cause slowdowns if is not a shared memory volume.
              volumeMounts:
                - mountPath: /dev/shm
                  name: dshm
                - mountPath: /home/ray/ray-results
                  name: ray-results
              resources:
                requests:
                  cpu: $worker_cpu
                  memory: $worker_mem
                limits:
                  cpu: $worker_cpu
                  # The maximum memory that this pod is allowed to use. The
                  # limit will be detected by ray and split to use 10% for
                  # redis, 30% for the shared memory object store, and the
                  # rest for application memory. If this limit is not set and
                  # the object store size is not set manually, ray will
                  # allocate a very large object store in each pod that may
                  # cause problems for other pods.
                  memory: $worker_mem
    - name: rayWorkerType
      minWorkers: $ray_worker_num
      maxWorkers: $ray_worker_num
      podConfig:
        apiVersion: v1
        kind: Pod
        metadata:
          generateName: ray-worker-type-
        spec:
          restartPolicy: Never
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumes:
            - name: dshm
              #nfs:
              #  server: 10.98.230.88
              #  path: /ray-dshm
              emptyDir: #{}
                medium: Memory
            - name: ray-results
              nfs:
                server: $nfs_server
                path: $nfs_path
          containers:
            - name: ray-node
              #imagePullPolicy: Always
              image: vdocker2603/ml-benchmark-raytune-k8s
              # Do not change this command - it keeps the pod alive until it is
              # explicitly killed.
              command: ["/bin/bash", "-c", "--"]
              args: ["trap : TERM INT; sleep infinity & wait;"]
              env:
                - name: RAY_gcs_server_rpc_server_thread_num
                  value: "1"
                - name: "METRICS_STORAGE_HOST"
                  value: "$metrics_ip"
              ports:
                - containerPort: 6379 # Redis port for Ray <= 1.10.0. GCS server port for Ray >= 1.11.0.
                - containerPort: 10001 # Used by Ray Client
                - containerPort: 8265 # Used by Ray Dashboard
                - containerPort: 8000 # Used by Ray Serve

              # This volume allocates shared memory for Ray to use for its plasma
              # object store. If you do not provide this, Ray will fall back to
              # /tmp which cause slowdowns if is not a shared memory volume.
              volumeMounts:
                - mountPath: /dev/shm
                  name: dshm
                - mountPath: /home/ray/ray-results
                  name: ray-results
              resources:
                requests:
                  cpu: $worker_cpu
                  memory: $worker_mem
                limits:
                  cpu: $worker_cpu
                  # The maximum memory that this pod is allowed to use. The
                  # limit will be detected by ray and split to use 10% for
                  # redis, 30% for the shared memory object store, and the
                  # rest for application memory. If this limit is not set and
                  # the object store size is not set manually, ray will
                  # allocate a very large object store in each pod that may
                  # cause problems for other pods.
                  memory: $worker_mem
  # Commands to start Ray on the head node. You don't need to change this.
  # Note dashboard-host is set to 0.0.0.0 so that Kubernetes can port forward.
  headStartRayCommands:
    - ray stop
    - ulimit -n 65536; ray start --head --port=6379 --no-monitor --dashboard-host 0.0.0.0
  # Commands to start Ray on worker nodes. You don't need to change this.
  workerStartRayCommands:
    - ray stop
    - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379
